<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AI Tutor â€” Voice Assistant</title>
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet" />
  <style>
    *,
    *::before,
    *::after {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }

    :root {
      --bg-primary: #0a0a1a;
      --bg-card: rgba(255, 255, 255, 0.04);
      --border: rgba(255, 255, 255, 0.08);
      --text-primary: #f0f0f5;
      --text-secondary: rgba(240, 240, 245, 0.55);
      --accent: #6c5ce7;
      --accent-glow: rgba(108, 92, 231, 0.35);
      --success: #00cec9;
      --danger: #ff6b6b;
      --danger-glow: rgba(255, 107, 107, 0.35);
    }

    body {
      font-family: 'Inter', -apple-system, sans-serif;
      background: var(--bg-primary);
      color: var(--text-primary);
      min-height: 100vh;
      display: flex;
      flex-direction: column;
      align-items: center;
      justify-content: center;
      overflow: hidden;
    }

    /* â”€â”€ Animated background â”€â”€ */
    .bg-orbs {
      position: fixed;
      inset: 0;
      z-index: 0;
      overflow: hidden;
      pointer-events: none;
    }

    .bg-orbs .orb {
      position: absolute;
      border-radius: 50%;
      filter: blur(100px);
      opacity: 0.15;
      animation: float 20s ease-in-out infinite;
    }

    .bg-orbs .orb:nth-child(1) {
      width: 500px;
      height: 500px;
      background: #6c5ce7;
      top: -15%;
      left: -10%;
      animation-delay: 0s;
    }

    .bg-orbs .orb:nth-child(2) {
      width: 400px;
      height: 400px;
      background: #00cec9;
      bottom: -10%;
      right: -5%;
      animation-delay: -7s;
    }

    .bg-orbs .orb:nth-child(3) {
      width: 300px;
      height: 300px;
      background: #fd79a8;
      top: 50%;
      left: 60%;
      animation-delay: -14s;
    }

    @keyframes float {

      0%,
      100% {
        transform: translate(0, 0) scale(1);
      }

      33% {
        transform: translate(30px, -40px) scale(1.05);
      }

      66% {
        transform: translate(-20px, 20px) scale(0.95);
      }
    }

    /* â”€â”€ Main container â”€â”€ */
    .container {
      position: relative;
      z-index: 1;
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 2.5rem;
      padding: 2rem;
      max-width: 420px;
      width: 100%;
    }

    /* â”€â”€ Header â”€â”€ */
    .header {
      text-align: center;
    }

    .header .logo {
      font-size: 2.8rem;
      margin-bottom: 0.5rem;
    }

    .header h1 {
      font-size: 1.75rem;
      font-weight: 700;
      letter-spacing: -0.02em;
      background: linear-gradient(135deg, #f0f0f5, #6c5ce7);
      -webkit-background-clip: text;
      -webkit-text-fill-color: transparent;
      background-clip: text;
    }

    .header p {
      font-size: 0.875rem;
      color: var(--text-secondary);
      margin-top: 0.35rem;
    }

    /* â”€â”€ Status card â”€â”€ */
    .status-card {
      width: 100%;
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 16px;
      padding: 1.25rem 1.5rem;
      backdrop-filter: blur(20px);
    }

    .status-row {
      display: flex;
      align-items: center;
      gap: 0.75rem;
    }

    .status-dot {
      width: 10px;
      height: 10px;
      border-radius: 50%;
      background: var(--text-secondary);
      flex-shrink: 0;
      transition: background 0.3s, box-shadow 0.3s;
    }

    .status-dot.connected {
      background: var(--success);
      box-shadow: 0 0 8px rgba(0, 206, 201, 0.5);
    }

    .status-dot.error {
      background: var(--danger);
      box-shadow: 0 0 8px var(--danger-glow);
    }

    .status-label {
      font-size: 0.8125rem;
      font-weight: 500;
      color: var(--text-secondary);
      transition: color 0.3s;
    }

    .status-label.active {
      color: var(--text-primary);
    }

    /* â”€â”€ Transcript area â”€â”€ */
    .transcript-area {
      width: 100%;
      min-height: 80px;
      max-height: 180px;
      overflow-y: auto;
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 16px;
      padding: 1rem 1.25rem;
      backdrop-filter: blur(20px);
      scrollbar-width: thin;
      scrollbar-color: rgba(255, 255, 255, 0.1) transparent;
    }

    .transcript-area p {
      font-size: 0.8125rem;
      color: var(--text-secondary);
      line-height: 1.7;
    }

    .transcript-area p.user-msg {
      color: var(--accent);
    }

    .transcript-area p.bot-msg {
      color: var(--success);
    }

    .transcript-placeholder {
      font-size: 0.8125rem;
      color: var(--text-secondary);
      text-align: center;
      opacity: 0.6;
    }

    /* â”€â”€ Mic button â”€â”€ */
    .mic-section {
      display: flex;
      flex-direction: column;
      align-items: center;
      gap: 1rem;
    }

    .mic-btn {
      position: relative;
      width: 88px;
      height: 88px;
      border-radius: 50%;
      border: none;
      cursor: pointer;
      background: linear-gradient(135deg, #636e72, #b2bec3);
      color: #fff;
      display: flex;
      align-items: center;
      justify-content: center;
      transition: transform 0.2s ease, box-shadow 0.3s ease, background 0.3s;
      box-shadow: 0 4px 20px rgba(99, 110, 114, 0.3);
      outline: none;
    }

    .mic-btn:hover {
      transform: scale(1.06);
    }

    .mic-btn:active {
      transform: scale(0.96);
    }

    .mic-btn.live {
      background: linear-gradient(135deg, #6c5ce7, #a29bfe);
      box-shadow: 0 4px 30px var(--accent-glow);
    }

    .mic-btn.live:hover {
      box-shadow: 0 6px 40px rgba(108, 92, 231, 0.5);
    }

    .mic-btn svg {
      width: 32px;
      height: 32px;
    }

    /* Pulse ring when live */
    .mic-btn::before {
      content: '';
      position: absolute;
      inset: -6px;
      border-radius: 50%;
      border: 2px solid rgba(108, 92, 231, 0.4);
      animation: pulse-ring 2s ease-out infinite;
      opacity: 0;
      transition: opacity 0.3s;
    }

    .mic-btn.live::before {
      opacity: 1;
    }

    @keyframes pulse-ring {
      0% {
        transform: scale(1);
        opacity: 0.6;
      }

      100% {
        transform: scale(1.5);
        opacity: 0;
      }
    }

    .mic-label {
      font-size: 0.75rem;
      font-weight: 500;
      color: var(--text-secondary);
      text-transform: uppercase;
      letter-spacing: 0.08em;
    }

    /* â”€â”€ Connect button â”€â”€ */
    .connect-btn {
      width: 100%;
      padding: 0.875rem;
      border: none;
      border-radius: 12px;
      background: linear-gradient(135deg, #6c5ce7, #a29bfe);
      color: #fff;
      font-family: inherit;
      font-size: 0.9375rem;
      font-weight: 600;
      cursor: pointer;
      transition: transform 0.15s, box-shadow 0.3s, opacity 0.3s;
      box-shadow: 0 4px 20px var(--accent-glow);
    }

    .connect-btn:hover {
      transform: translateY(-1px);
      box-shadow: 0 6px 30px rgba(108, 92, 231, 0.5);
    }

    .connect-btn:active {
      transform: translateY(0);
    }

    .connect-btn:disabled {
      opacity: 0.5;
      cursor: not-allowed;
      transform: none;
    }

    .connect-btn.disconnect {
      background: linear-gradient(135deg, #ff6b6b, #ee5a5a);
      box-shadow: 0 4px 20px var(--danger-glow);
    }

    /* â”€â”€ Audio visualizer â”€â”€ */
    .visualizer {
      display: flex;
      align-items: center;
      justify-content: center;
      gap: 3px;
      height: 24px;
      opacity: 0;
      transition: opacity 0.3s;
    }

    .visualizer.active {
      opacity: 1;
    }

    .visualizer .bar {
      width: 3px;
      height: 4px;
      background: var(--accent);
      border-radius: 2px;
      transition: height 0.1s;
    }

    /* â”€â”€ Footer â”€â”€ */
    .footer {
      position: fixed;
      bottom: 1.5rem;
      font-size: 0.6875rem;
      color: var(--text-secondary);
      opacity: 0.5;
      text-align: center;
    }
  </style>
</head>

<body>
  <div class="bg-orbs">
    <div class="orb"></div>
    <div class="orb"></div>
    <div class="orb"></div>
  </div>

  <div class="container">
    <div class="header">
      <div class="logo">ðŸŽ“</div>
      <h1>AI Tutor</h1>
      <p>Powered by Sarvam AI & Gemini</p>
    </div>

    <div class="status-card">
      <div class="status-row">
        <div class="status-dot" id="statusDot"></div>
        <span class="status-label" id="statusLabel">Ready to connect</span>
      </div>
    </div>

    <div class="transcript-area" id="transcriptArea">
      <p class="transcript-placeholder" id="placeholder">
        Conversation will appear hereâ€¦
      </p>
    </div>

    <div class="visualizer" id="visualizer">
      <div class="bar"></div>
      <div class="bar"></div>
      <div class="bar"></div>
      <div class="bar"></div>
      <div class="bar"></div>
      <div class="bar"></div>
      <div class="bar"></div>
      <div class="bar"></div>
      <div class="bar"></div>
    </div>

    <div class="mic-section">
      <button class="mic-btn" id="micBtn" disabled title="Toggle microphone">
        <svg id="micOnIcon" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke="currentColor"
          stroke-width="2">
          <path stroke-linecap="round" stroke-linejoin="round" d="M12 1a4 4 0 00-4 4v6a4 4 0 008 0V5a4 4 0 00-4-4z" />
          <path stroke-linecap="round" stroke-linejoin="round" d="M19 10v1a7 7 0 01-14 0v-1" />
          <line x1="12" y1="19" x2="12" y2="23" />
          <line x1="8" y1="23" x2="16" y2="23" />
        </svg>
        <svg id="micOffIcon" style="display:none" xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24"
          stroke="currentColor" stroke-width="2">
          <line x1="1" y1="1" x2="23" y2="23" />
          <path stroke-linecap="round" stroke-linejoin="round"
            d="M9 9v2a3 3 0 005.12 2.12M15 9.34V4a3 3 0 00-5.94-.6" />
          <path stroke-linecap="round" stroke-linejoin="round"
            d="M17 16.95A7 7 0 015 12v-2m14 0v2c0 .76-.12 1.5-.35 2.18" />
          <line x1="12" y1="19" x2="12" y2="23" />
          <line x1="8" y1="23" x2="16" y2="23" />
        </svg>
      </button>
      <span class="mic-label" id="micLabel">Mic Off</span>
    </div>

    <button class="connect-btn" id="connectBtn">Connect to Tutor</button>
  </div>

  <div class="footer">Sarvam AI Ã— Gemini 2.5 Pro Ã— Pipecat</div>

  <!-- Hidden audio element for bot voice output -->
  <audio id="botAudio" autoplay></audio>

  <script>
    // â”€â”€ DOM refs â”€â”€
    const micBtn = document.getElementById('micBtn');
    const micOnIcon = document.getElementById('micOnIcon');
    const micOffIcon = document.getElementById('micOffIcon');
    const micLabel = document.getElementById('micLabel');
    const connectBtn = document.getElementById('connectBtn');
    const statusDot = document.getElementById('statusDot');
    const statusLabel = document.getElementById('statusLabel');
    const transcriptArea = document.getElementById('transcriptArea');
    const placeholder = document.getElementById('placeholder');
    const visualizer = document.getElementById('visualizer');
    const bars = visualizer.querySelectorAll('.bar');
    const botAudio = document.getElementById('botAudio');

    // â”€â”€ State â”€â”€
    let peerConnection = null;
    let localStream = null;
    let isMuted = true;
    let isConnected = false;
    let vizInterval = null;

    // â”€â”€ Helpers â”€â”€
    function setStatus(text, state) {
      statusLabel.textContent = text;
      statusDot.className = 'status-dot' + (state ? ' ' + state : '');
      statusLabel.className = 'status-label' + (state === 'connected' ? ' active' : '');
    }

    function addTranscript(text, role) {
      placeholder.style.display = 'none';
      const p = document.createElement('p');
      p.textContent = (role === 'user' ? 'ðŸŽ¤ You: ' : 'ðŸ¤– Tutor: ') + text;
      p.className = role === 'user' ? 'user-msg' : 'bot-msg';
      transcriptArea.appendChild(p);
      transcriptArea.scrollTop = transcriptArea.scrollHeight;
    }

    function startViz() {
      visualizer.classList.add('active');
      vizInterval = setInterval(function () {
        bars.forEach(function (bar) {
          bar.style.height = Math.random() * 20 + 4 + 'px';
        });
      }, 120);
    }

    function stopViz() {
      visualizer.classList.remove('active');
      clearInterval(vizInterval);
      bars.forEach(function (bar) { bar.style.height = '4px'; });
    }

    function updateMicUI() {
      if (isMuted) {
        micBtn.classList.remove('live');
        micOnIcon.style.display = 'none';
        micOffIcon.style.display = 'block';
        micLabel.textContent = 'Mic Off';
        stopViz();
      } else {
        micBtn.classList.add('live');
        micOnIcon.style.display = 'block';
        micOffIcon.style.display = 'none';
        micLabel.textContent = 'Mic On â€” Speak now';
        startViz();
      }
    }

    // â”€â”€ Mic toggle â”€â”€
    micBtn.addEventListener('click', function () {
      if (!isConnected || !localStream) return;
      isMuted = !isMuted;
      // Mute/unmute the audio track
      localStream.getAudioTracks().forEach(function (track) {
        track.enabled = !isMuted;
      });
      updateMicUI();
      console.log('Mic ' + (isMuted ? 'muted' : 'unmuted'));
    });

    // â”€â”€ Connect / Disconnect â”€â”€
    connectBtn.addEventListener('click', function () {
      if (isConnected) {
        disconnect();
      } else {
        connect();
      }
    });

    async function connect() {
      connectBtn.disabled = true;
      connectBtn.textContent = 'Connectingâ€¦';
      setStatus('Requesting mic accessâ€¦', '');

      try {
        // 1. Get microphone access
        localStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        });

        // Start muted
        localStream.getAudioTracks().forEach(function (track) {
          track.enabled = false;
        });
        isMuted = true;

        setStatus('Creating connectionâ€¦', '');

        // 2. Create RTCPeerConnection
        peerConnection = new RTCPeerConnection({
          iceServers: [{ urls: 'stun:stun.l.google.com:19302' }]
        });

        // Add transceiver to force audio direction
        peerConnection.addTransceiver('audio', { direction: 'sendrecv' });

        // Add local audio track
        localStream.getTracks().forEach(function (track) {
          peerConnection.addTrack(track, localStream);
        });

        // Handle remote audio (bot's voice)
        peerConnection.ontrack = function (event) {
          console.log('Received remote track:', event.track.kind);
          if (event.streams && event.streams[0]) {
            botAudio.srcObject = event.streams[0];
          }
        };

        // Log ICE state changes
        peerConnection.oniceconnectionstatechange = function () {
          console.log('ICE state:', peerConnection.iceConnectionState);
          if (peerConnection.iceConnectionState === 'connected') {
            setStatus('Connected â€” unmute mic to speak', 'connected');
          } else if (peerConnection.iceConnectionState === 'disconnected' ||
            peerConnection.iceConnectionState === 'failed') {
            setStatus('Connection lost', 'error');
            disconnect();
          }
        };

        // 3. Create offer
        // Create Data Channel for side-channel info (status, etc.)
        const dataChannel = peerConnection.createDataChannel("pipecat-data");

        dataChannel.onopen = () => {
          console.log("Data channel open");
        };

        dataChannel.onmessage = (event) => {
          try {
            const msg = JSON.parse(event.data);
            console.log("Received data channel message:", msg); // Debug log
            if (msg.type === "status") {
              if (msg.status === "processing") {
                setStatus("Thinkingâ€¦", "connected");
                stopViz(); // Stop mic visualizer
                // Optional: Add specific "thinking" visual state here
              } else if (msg.status === "speaking") {
                setStatus("Speakingâ€¦", "connected");
                // We could start a different visualizer for the bot, 
                // but for now let's just show text.
                // The audio element is autoplay, so we hear it.
              } else if (msg.status === "listening") {
                setStatus("Listening â€” speak now", "connected");
                if (!isMuted) startViz();
              }
            } else if (msg.type === "user-transcription" || (msg.type === "rtvi-ai" && msg.label === "rtvi-ai" && msg.type === "user-transcription")) {
              // Handle user transcription (from STT)
              // Format might be msg.data.text or msg.text depending on the frame
              let text = msg.text || (msg.data && msg.data.text);
              if (text) {
                updateTranscript("You", text, true);
              }
            } else if (msg.type === "assistant-transcription" || (msg.type === "rtvi-ai" && msg.label === "rtvi-ai" && msg.type === "bot-transcription")) {
              // Handle bot transcription (from LLM/TTS)
              let text = msg.text || (msg.data && msg.data.text);
              if (text) {
                updateTranscript("Tutor", text, true);
              }
            } else {
              // Check if it's the specific structure from the logs:
              // {'label': 'rtvi-ai', 'type': 'user-transcription', 'data': {'text': '...', ...}}
              if (msg.label === "rtvi-ai") {
                if (msg.type === "user-transcription" && msg.data && msg.data.text) {
                  updateTranscript("You", msg.data.text, true);
                }
              }
            }
          } catch (e) {
            console.error("Error parsing data channel message:", e);
          }
        };

        function updateTranscript(speaker, text, isFinal) {
          const transcript = document.getElementById('transcriptArea'); // Changed to transcriptArea
          if (placeholder) { // Check if placeholder exists
            placeholder.style.display = 'none'; // Hide placeholder
          }

          // Check if the last element is from the same speaker and not final?
          // For now, just append.
          const p = document.createElement('p');
          p.innerHTML = `<strong>${speaker}:</strong> ${text}`;
          transcript.appendChild(p);
          transcript.scrollTop = transcript.scrollHeight;
        }

        var offer = await peerConnection.createOffer({
          offerToReceiveAudio: true
        });
        await peerConnection.setLocalDescription(offer);

        // Wait for ICE gathering to complete (or timeout)
        await waitForICEGathering(peerConnection, 3000);

        setStatus('Connecting to tutorâ€¦', '');

        // 4. Send offer to server, get answer
        var response = await fetch('/api/offer', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            sdp: peerConnection.localDescription.sdp,
            type: peerConnection.localDescription.type
          })
        });

        if (!response.ok) {
          var errData = await response.json().catch(function () { return {}; });
          throw new Error(errData.error || 'Server error ' + response.status);
        }

        var answer = await response.json();

        // 5. Set remote description
        await peerConnection.setRemoteDescription(
          new RTCSessionDescription({ sdp: answer.sdp, type: answer.type })
        );

        // Connected!
        isConnected = true;
        setStatus('Connected â€” unmute mic to speak', 'connected');
        connectBtn.disabled = false;
        connectBtn.textContent = 'Disconnect';
        connectBtn.classList.add('disconnect');
        micBtn.disabled = false;
        updateMicUI();

        console.log('WebRTC connection established');

      } catch (err) {
        console.error('Connection failed:', err);
        setStatus('Error: ' + err.message, 'error');
        connectBtn.disabled = false;
        connectBtn.textContent = 'Retry Connection';
        connectBtn.classList.remove('disconnect');
        cleanup();
      }
    }

    function waitForICEGathering(pc, timeoutMs) {
      return new Promise(function (resolve) {
        if (pc.iceGatheringState === 'complete') {
          resolve();
          return;
        }
        var timeout = setTimeout(function () {
          console.log('ICE gathering timed out, proceeding with available candidates');
          resolve();
        }, timeoutMs);

        pc.onicegatheringstatechange = function () {
          if (pc.iceGatheringState === 'complete') {
            clearTimeout(timeout);
            resolve();
          }
        };
      });
    }

    function disconnect() {
      cleanup();
      isConnected = false;
      isMuted = true;
      setStatus('Disconnected', '');
      connectBtn.textContent = 'Connect to Tutor';
      connectBtn.classList.remove('disconnect');
      connectBtn.disabled = false;
      micBtn.disabled = true;
      micBtn.classList.remove('live');
      micOnIcon.style.display = 'block';
      micOffIcon.style.display = 'none';
      micLabel.textContent = 'Mic Off';
      stopViz();
      console.log('Disconnected');
    }

    function cleanup() {
      if (peerConnection) {
        peerConnection.close();
        peerConnection = null;
      }
      if (localStream) {
        localStream.getTracks().forEach(function (track) { track.stop(); });
        localStream = null;
      }
      botAudio.srcObject = null;
    }

    // Log that script loaded
    console.log('ðŸŽ“ AI Tutor UI loaded');
  </script>
</body>

</html>